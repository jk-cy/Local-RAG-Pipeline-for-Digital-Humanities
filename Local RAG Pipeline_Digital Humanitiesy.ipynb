{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59974905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PDF\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# your pdf path\n",
    "pdf_path = \"/sample_data/(2010) Kirschenbaum - What is Digital Humanities.pdf\"\n",
    "print(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF onine if not availale locally\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"[INFO] File doesn't exist, downloading...\")\n",
    "\n",
    "    # Enter URL of PDF\n",
    "    url = \"https://mkirschenbaum.wordpress.com/wp-content/uploads/2011/03/ade-final.pdf\"\n",
    "\n",
    "    # the local filename to save the download file\n",
    "    filename = pdf_path\n",
    "\n",
    "    # send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # check if the request was successful\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # open the file and save it\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[INFO] Failed to download the file. Status code: {response.status_code}\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF file, see: http://github.com/pymupdf/PyMuPDF\n",
    "import fitz\n",
    "\n",
    "print(\"PyMuPDF is installed and the fitz module is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec93dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    # more text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append(\n",
    "            {\n",
    "                \"page_number\": page_number\n",
    "                - 5,  # adjust page numbers since our PDF starts on page 42\n",
    "                \"page_char_count\": len(text),\n",
    "                \"page_word_count\": len(text.split(\" \")),\n",
    "                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                \"page_token_count\": len(text)\n",
    "                / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                \"text\": text,\n",
    "            }\n",
    "        )\n",
    "    return pages_and_texts\n",
    "\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445fd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading some stats from the text\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a78618",
   "metadata": {},
   "source": [
    "#### Note: The Importance of Token Count\n",
    "\n",
    "It’s essential to consider token count when working with embedding models or Large Language Models (LLMs), as they cannot process an infinite number of tokens. During tokenization, some information may be lost or truncated, which can impact the quality of the embeddings and the model’s understanding of the text. Carefully managing token counts is crucial, especially for applications requiring detailed analysis, such as close reading of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3fec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e62775ad",
   "metadata": {},
   "source": [
    "#### Further Text Processing: Splitting Pages into Sentences\n",
    "\n",
    "1. Splitting on \".\"\n",
    "2. This step can be done with spaCy or nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00330916-e27f-4d7e-9bb4-6e3ddb5001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fd842",
   "metadata": {},
   "source": [
    "Depending on your corpus, you may choose which spaCy model to use:\n",
    "\n",
    "**Small model**\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "**Medium model**\n",
    "python -m spacy download en_core_web_md\n",
    "\n",
    "**Large model**\n",
    "python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd85201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, c.f. https://spacy.io/api/sentencizer\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create document instance as an example\n",
    "doc = nlp(\n",
    "    \"This is a sentence. This is another sentence. This is about digital humanities.\"\n",
    ")\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# Print out sentences split\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b66bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b59b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Ensure all sentences are strings (currently spaCy datatype)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # count the sentences\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558309c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f511307",
   "metadata": {},
   "source": [
    "#### Chunking Text for Embedding and Retrieval\n",
    "\n",
    "Chunking text into manageable segments is essential for effective processing in a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "**LangChain** provides tools to handle this chunking, making it easier to organize content to fit within the embedding model’s context window. Proper chunking ensures that the context passed to the LLM is focused and relevant, leading to more accurate and precise responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6821e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10  # Experiment with the number\n",
    "\n",
    "\n",
    "# Create a funcion to split lists of texts recursively into chunk size\n",
    "# e.g. [20]->[10,10] or [25] -> [10,10,5]\n",
    "def split_list(\n",
    "    input_list: list[str], slice_size: int = num_sentence_chunk_size\n",
    ") -> list[list[str]]:\n",
    "    return [\n",
    "        input_list[i : i + slice_size] for i in range(0, len(input_list), slice_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd09a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop throough ppers and texts and split sentencds into chunk\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(\n",
    "        input_list=item[\"sentences\"], slice_size=num_sentence_chunk_size\n",
    "    )\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef9e8f",
   "metadata": {},
   "source": [
    "### Splitting and Embedding Text Chunks\n",
    "\n",
    "Each chunk of text is split into individual items and embedded as its own unique numerical representation. This process helps ensure that each segment of content is distinct and appropriately prepared for retrieval and analysis in the RAG pipeline.\n",
    "\n",
    "The following code performs this function by:\n",
    "\n",
    "1. **Combining Sentences into Chunks**: Each chunk is created by joining related sentences into a paragraph-like structure.\n",
    "2. **Cleaning and Formatting**: Simple formatting (e.g., adding spaces after full stops) ensures consistency and readability within each chunk.\n",
    "3. **Generating Statistics**: The code calculates character count, word count, and an estimated token count for each chunk, which can be useful for managing context windows and model constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e30b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# split each chunk into its own item\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(\n",
    "            r\"\\.([A-Z])\", r\". \\1\", joined_sentence_chunk\n",
    "        )  # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len(\n",
    "            [word for word in joined_sentence_chunk.split(\" \")]\n",
    "        )\n",
    "        chunk_dict[\"chunk_token_count\"] = (\n",
    "            len(joined_sentence_chunk) / 4\n",
    "        )  # 1 token = ~4 characters\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# Number of chunks we have\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c038ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e50ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())  # Displays the first few rows of the DataFrame\n",
    "print(\n",
    "    df[\"chunk_token_count\"].describe()\n",
    ")  # Shows statistics for the chunk_token_count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter chunks of text for short chunks as they may not contain much useful information\n",
    "# Show random chunks with under 70 tokens in length\n",
    "# You may need to adjust the number of tokens. The below code has been adjusted to check whether tokens of your expected count exists in the DatafFrame.\n",
    "\n",
    "# Set min_token_length to filter out short chunks\n",
    "min_token_length = 70\n",
    "\n",
    "# Filter and check the DataFrame\n",
    "filtered_df = df[df[\"chunk_token_count\"] <= min_token_length]\n",
    "print(\n",
    "    f\"Number of chunks found with token count <= {min_token_length}: {len(filtered_df)}\"\n",
    ")\n",
    "\n",
    "if not filtered_df.empty:\n",
    "    sample_size = min(5, len(filtered_df))\n",
    "    for _, row in filtered_df.sample(sample_size).iterrows():\n",
    "        print(\n",
    "            f'Chunk token count: {row[\"chunk_token_count\"]} | Text: {row[\"sentence_chunk\"]}'\n",
    "        )\n",
    "else:\n",
    "    print(f\"No chunks found with token count <= {min_token_length}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b299ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Dataframe for rows with under 70 tokens\n",
    "# You will need to adjust the number of tokens depending on your DataFrame structure\n",
    "\n",
    "pages_and_chunks_over_min_token_len = df[\n",
    "    df[\"chunk_token_count\"] > min_token_length\n",
    "].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e385295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_chunks_over_min_token_len, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45649f6d",
   "metadata": {},
   "source": [
    "#### Embedding Text Chunks\n",
    "\n",
    "see https://vickiboykis.com/what_are_embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd183e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Downgraded NumPy to version <2 for compatibility with sentence-transformers.\n",
    "# NumPy 2.x versions are currently not fully supported by sentence-transformers and may cause errors.\n",
    "# This downgrade ensures stable integration when encoding embeddings.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    model_name_or_path=\"all-mpnet-base-v2\", device=\"cpu\"\n",
    ")  # Set device to \"cpu\" or \"cuda\" depending on available hardware and speed\n",
    "\n",
    "\n",
    "# Create the list of sentences\n",
    "sentences = [\n",
    "    \"The Sentence Transformer library provides an easy way to create embeddings.\",\n",
    "    \"Sentence embedding is part of the process.\",\n",
    "    \"This is a digital humanities project.\",\n",
    "]\n",
    "\n",
    "# encode sentences by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embedding_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# check the embeddings\n",
    "for sentence, embedding in embedding_dict.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b220bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Move the model to the CPU for embedding creation\n",
    "embedding_model.to(\"cpu\")\n",
    "\n",
    "# Generate embeddings for each chunk on the CPU\n",
    "# This iterates over each item in pages_and_chunks_over_min_token_len\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time taken for CPU embedding creation: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Optional: Move the model to the GPU for faster embedding creation\n",
    "# Uncomment the following lines if a GPU is available for use with \"cuda\"\n",
    "\n",
    "# embedding_model.to(\"cuda\")  # Requires a GPU to be installed\n",
    "\n",
    "# # Start timing for GPU embedding\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Create embeddings on the GPU (if available)\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "\n",
    "# # Calculate and print the elapsed time for GPU processing\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Time taken for GPU embedding creation: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4938a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Measure execution time for batch embedding and saving (if in Jupyter, uncomment %%time)\n",
    "# %%time\n",
    "\n",
    "# Embed all text chunks in batches to optimize memory and performance\n",
    "# Uncomment the following to batch embeddings:\n",
    "# text_chunk_embeddings = embedding_model.encode(\n",
    "#     text_chunks,  # List of text chunks to embed\n",
    "#     batch_size=32,  # Adjust batch size as needed to improve speed and memory efficiency\n",
    "#     convert_to_tensor=True  # Optional: return embeddings as PyTorch tensor for further tensor operations\n",
    "# )\n",
    "\n",
    "# Optional: Display or use `text_chunk_embeddings` for further processing\n",
    "\n",
    "# Save the text chunks and embeddings to a DataFrame\n",
    "# Note: Adjust path as needed\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(\n",
    "    embeddings_df_save_path, index=False\n",
    ")  # Save DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713f977",
   "metadata": {},
   "source": [
    "### Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ebf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "    \"embedding\"\n",
    "].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(\n",
    "    np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32\n",
    ").to(device)\n",
    "embeddings.shape\n",
    "embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdf425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6aaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(text_chunks_and_embedding_df[\"embedding\"].tolist(), axis=0)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    model_name_or_path=\"all-mpnet-base-v2\", device=device\n",
    ")  # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2cbdf0",
   "metadata": {},
   "source": [
    "### Steps for Querying with Embeddings\n",
    "\n",
    "1. **Define a Query String**: Start by creating a query as a text string that represents what you’re searching for.\n",
    "\n",
    "2. **Convert the Query to an Embedding**: Use the same embedding model to transform the query string into an embedding, similar to how the text chunks were embedded.\n",
    "\n",
    "3. **Calculate Similarity**: Perform a similarity comparison (e.g., dot product or cosine similarity) between the query embedding and each text embedding in the dataset.\n",
    "\n",
    "4. **Sort Results by Relevance**: Sort the similarity scores in descending order to identify the most relevant results for your query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "# 1. Define the query\n",
    "query = \"digital humanities\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples\n",
    "# Ensure that both query_embedding and embeddings are in the same format and dtype\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Convert embeddings to a PyTorch tensor if it's currently a NumPy array\n",
    "if isinstance(embeddings, np.ndarray):\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# 3. Get similarity scores with the dot product\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(\n",
    "    f\"Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\"\n",
    ")\n",
    "\n",
    "# 4. Get the top-k results (keeping this to 5 for now)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the output by inputting a number from the indices\n",
    "pages_and_chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_embeddings = torch.randn(100 * embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape:  {larger_embeddings.shape}\")\n",
    "\n",
    "# Perform dot product across 168,000 embeddings\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
    "embeddings.dtype\n",
    "end_time = timer()\n",
    "\n",
    "print(\n",
    "    f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
